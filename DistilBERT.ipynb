{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a8d07e1-85b0-450a-9984-607c317dec2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.21.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.23.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2019.11.28)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5655320-b78a-47ea-b14e-ddb459251ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.9/dist-packages (2.9.2)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.21.3)\n",
      "Requirement already satisfied: keras in /usr/local/lib/python3.9/dist-packages (2.9.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.8.0)\n",
      "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.9.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow) (66.1.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.14.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.30.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.19.6)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow) (23.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (15.0.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (4.4.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.23.4)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.51.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.12.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow) (0.35.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (5.3.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (6.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow) (2.1.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.2.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow transformers keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e4b646e-eced-4ff7-bc5a-261bdeb07475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six in /usr/lib/python3/dist-packages (from langdetect) (1.14.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993225 sha256=9a7d2f8033a516008aaffde4d7706774a352aeb421f85c2afa98c0ee13ccad78\n",
      "  Stored in directory: /root/.cache/pip/wheels/6a/67/f8/9cf1a8ff87e0b37f738769df49cc142a655489a6d27b68089f\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eedbd133-ff15-4cb1-a0ca-a42a12b93a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wordcloud\n",
      "  Downloading wordcloud-1.9.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (460 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m460.4/460.4 kB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from wordcloud) (3.6.1)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.9/dist-packages (from wordcloud) (1.23.4)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.9/dist-packages (from wordcloud) (9.2.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->wordcloud) (4.38.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->wordcloud) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->wordcloud) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->wordcloud) (23.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->wordcloud) (1.0.7)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.14.0)\n",
      "Installing collected packages: wordcloud\n",
      "Successfully installed wordcloud-1.9.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a95e1f9f-7efb-429b-863e-e7b78e814d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting geopy\n",
      "  Downloading geopy-2.3.0-py3-none-any.whl (119 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting geographiclib<3,>=1.52\n",
      "  Downloading geographiclib-2.0-py3-none-any.whl (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: geographiclib, geopy\n",
      "Successfully installed geographiclib-2.0 geopy-2.3.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24865e5e-8d48-4421-b013-6a8a1d326c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting geopandas\n",
      "  Downloading geopandas-0.13.0-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting shapely>=1.7.1\n",
      "  Downloading shapely-2.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m115.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyproj>=3.0.1\n",
      "  Downloading pyproj-3.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m99.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting fiona>=1.8.19\n",
      "  Downloading Fiona-1.9.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from geopandas) (23.0)\n",
      "Requirement already satisfied: pandas>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from geopandas) (1.5.0)\n",
      "Requirement already satisfied: click~=8.0 in /usr/local/lib/python3.9/dist-packages (from fiona>=1.8.19->geopandas) (8.1.3)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.9/dist-packages (from fiona>=1.8.19->geopandas) (6.0.0)\n",
      "Collecting cligj>=0.5\n",
      "  Downloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from fiona>=1.8.19->geopandas) (1.14.0)\n",
      "Requirement already satisfied: certifi in /usr/lib/python3/dist-packages (from fiona>=1.8.19->geopandas) (2019.11.28)\n",
      "Collecting attrs>=19.2.0\n",
      "  Downloading attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting click-plugins>=1.0\n",
      "  Downloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.1.0->geopandas) (2022.7.1)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.1.0->geopandas) (1.23.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.1.0->geopandas) (2.8.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata->fiona>=1.8.19->geopandas) (3.11.0)\n",
      "Installing collected packages: shapely, pyproj, cligj, click-plugins, attrs, fiona, geopandas\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 18.2.0\n",
      "    Uninstalling attrs-18.2.0:\n",
      "      Successfully uninstalled attrs-18.2.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gradient 2.0.6 requires attrs<=19, but you have attrs 23.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed attrs-23.1.0 click-plugins-1.1.1 cligj-0.7.2 fiona-1.9.4 geopandas-0.13.0 pyproj-3.5.0 shapely-2.0.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95d988a9-db0a-41c6-9cb0-15037a27f8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import spacy\n",
    "import random\n",
    "\n",
    "from langdetect import detect\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "from geopy.geocoders import Nominatim\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import BertForSequenceClassification, BertModel, BertTokenizer, TFBertModel\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords as stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8f83ca3-f638-4292-a7e4-73c07fe14b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('merged_cluster100.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3418510-e04f-408d-99bd-a3a15afea348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>cluster_id</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>language_label</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>second</th>\n",
       "      <th>cluster_label</th>\n",
       "      <th>distance_from_center</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>wow ’ know homies coming . lit .</td>\n",
       "      <td>5.853807e+07</td>\n",
       "      <td>2021-08-02 01:25:46</td>\n",
       "      <td>2</td>\n",
       "      <td>34.020789</td>\n",
       "      <td>-118.411907</td>\n",
       "      <td>0</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>1</td>\n",
       "      <td>13.970579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>foggy nights peaceful calming chill vibe</td>\n",
       "      <td>1.032647e+18</td>\n",
       "      <td>2021-08-28 08:47:36</td>\n",
       "      <td>2</td>\n",
       "      <td>34.020789</td>\n",
       "      <td>-118.411907</td>\n",
       "      <td>0</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>1</td>\n",
       "      <td>13.970579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>khamisvideobomb</td>\n",
       "      <td>2.781174e+07</td>\n",
       "      <td>2021-08-13 15:49:15</td>\n",
       "      <td>2</td>\n",
       "      <td>34.020789</td>\n",
       "      <td>-118.411907</td>\n",
       "      <td>24</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1</td>\n",
       "      <td>13.970579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>anyone tryna hit gym 4 hmu</td>\n",
       "      <td>5.490040e+08</td>\n",
       "      <td>2021-08-31 22:41:20</td>\n",
       "      <td>2</td>\n",
       "      <td>34.020789</td>\n",
       "      <td>-118.411907</td>\n",
       "      <td>24</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>13.970579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>kidding intro crazy .</td>\n",
       "      <td>2.963951e+07</td>\n",
       "      <td>2021-08-06 03:44:29</td>\n",
       "      <td>2</td>\n",
       "      <td>34.020789</td>\n",
       "      <td>-118.411907</td>\n",
       "      <td>4</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1</td>\n",
       "      <td>13.970579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604201</th>\n",
       "      <td>604201</td>\n",
       "      <td>students must dressed appropriately day arrive...</td>\n",
       "      <td>1.990241e+08</td>\n",
       "      <td>2021-08-12 11:08:07</td>\n",
       "      <td>2996</td>\n",
       "      <td>29.668301</td>\n",
       "      <td>-95.158936</td>\n",
       "      <td>0</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4</td>\n",
       "      <td>33.786445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604202</th>\n",
       "      <td>604202</td>\n",
       "      <td>actually hate working lunch shift crew job</td>\n",
       "      <td>1.382980e+18</td>\n",
       "      <td>2021-08-04 15:35:10</td>\n",
       "      <td>2996</td>\n",
       "      <td>29.668301</td>\n",
       "      <td>-95.158936</td>\n",
       "      <td>0</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4</td>\n",
       "      <td>33.786445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604203</th>\n",
       "      <td>604203</td>\n",
       "      <td>’ reading thisyo mama</td>\n",
       "      <td>1.039173e+18</td>\n",
       "      <td>2021-08-24 00:49:46</td>\n",
       "      <td>2996</td>\n",
       "      <td>29.668301</td>\n",
       "      <td>-95.158936</td>\n",
       "      <td>24</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>4</td>\n",
       "      <td>33.786445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604204</th>\n",
       "      <td>604204</td>\n",
       "      <td>mean secondddd</td>\n",
       "      <td>1.046619e+18</td>\n",
       "      <td>2021-08-02 17:02:13</td>\n",
       "      <td>2996</td>\n",
       "      <td>29.668301</td>\n",
       "      <td>-95.158936</td>\n",
       "      <td>0</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>4</td>\n",
       "      <td>33.786445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604205</th>\n",
       "      <td>604205</td>\n",
       "      <td>mixing different kinds tide pods downy unstopp...</td>\n",
       "      <td>1.039173e+18</td>\n",
       "      <td>2021-08-17 20:02:40</td>\n",
       "      <td>2996</td>\n",
       "      <td>29.668301</td>\n",
       "      <td>-95.158936</td>\n",
       "      <td>0</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>4</td>\n",
       "      <td>33.786445</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>604206 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                               text  \\\n",
       "0            0                   wow ’ know homies coming . lit .   \n",
       "1            1           foggy nights peaceful calming chill vibe   \n",
       "2            2                                    khamisvideobomb   \n",
       "3            3                         anyone tryna hit gym 4 hmu   \n",
       "4            4                              kidding intro crazy .   \n",
       "...        ...                                                ...   \n",
       "604201  604201  students must dressed appropriately day arrive...   \n",
       "604202  604202         actually hate working lunch shift crew job   \n",
       "604203  604203                              ’ reading thisyo mama   \n",
       "604204  604204                                     mean secondddd   \n",
       "604205  604205  mixing different kinds tide pods downy unstopp...   \n",
       "\n",
       "             user_id            timestamp  cluster_id        lat         lng  \\\n",
       "0       5.853807e+07  2021-08-02 01:25:46           2  34.020789 -118.411907   \n",
       "1       1.032647e+18  2021-08-28 08:47:36           2  34.020789 -118.411907   \n",
       "2       2.781174e+07  2021-08-13 15:49:15           2  34.020789 -118.411907   \n",
       "3       5.490040e+08  2021-08-31 22:41:20           2  34.020789 -118.411907   \n",
       "4       2.963951e+07  2021-08-06 03:44:29           2  34.020789 -118.411907   \n",
       "...              ...                  ...         ...        ...         ...   \n",
       "604201  1.990241e+08  2021-08-12 11:08:07        2996  29.668301  -95.158936   \n",
       "604202  1.382980e+18  2021-08-04 15:35:10        2996  29.668301  -95.158936   \n",
       "604203  1.039173e+18  2021-08-24 00:49:46        2996  29.668301  -95.158936   \n",
       "604204  1.046619e+18  2021-08-02 17:02:13        2996  29.668301  -95.158936   \n",
       "604205  1.039173e+18  2021-08-17 20:02:40        2996  29.668301  -95.158936   \n",
       "\n",
       "        language_label    year  month   day  hour  minute  second  \\\n",
       "0                    0  2021.0    8.0   2.0   1.0    25.0    46.0   \n",
       "1                    0  2021.0    8.0  28.0   8.0    47.0    36.0   \n",
       "2                   24  2021.0    8.0  13.0  15.0    49.0    15.0   \n",
       "3                   24  2021.0    8.0  31.0  22.0    41.0    20.0   \n",
       "4                    4  2021.0    8.0   6.0   3.0    44.0    29.0   \n",
       "...                ...     ...    ...   ...   ...     ...     ...   \n",
       "604201               0  2021.0    8.0  12.0  11.0     8.0     7.0   \n",
       "604202               0  2021.0    8.0   4.0  15.0    35.0    10.0   \n",
       "604203              24  2021.0    8.0  24.0   0.0    49.0    46.0   \n",
       "604204               0  2021.0    8.0   2.0  17.0     2.0    13.0   \n",
       "604205               0  2021.0    8.0  17.0  20.0     2.0    40.0   \n",
       "\n",
       "        cluster_label  distance_from_center  \n",
       "0                   1             13.970579  \n",
       "1                   1             13.970579  \n",
       "2                   1             13.970579  \n",
       "3                   1             13.970579  \n",
       "4                   1             13.970579  \n",
       "...               ...                   ...  \n",
       "604201              4             33.786445  \n",
       "604202              4             33.786445  \n",
       "604203              4             33.786445  \n",
       "604204              4             33.786445  \n",
       "604205              4             33.786445  \n",
       "\n",
       "[604206 rows x 16 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c729df0d-2eaf-4737-8404-c7ac0c61e490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Take a random sample size of merged dataset\n",
    "# sample_size = 20000\n",
    "# df = df.sample(n=sample_size)\n",
    "# df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f77a693-2547-4e8f-a386-d2e5f2d2faa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13202efe-c6bc-4c7a-966e-e9bef4bb1049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Prepare your data\n",
    "# Assuming you have your data stored in a DataFrame called 'df'\n",
    "df['text'] = df['text'].fillna(' ')\n",
    "text_data = df['text'].tolist()\n",
    "location_labels = df[['lat', 'lng']].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbbd98ba-27d1-43a0-809c-3ef378514f05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f59857adda84493d819bfc86d0ee6783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3b3de19496e42a6aadcb0e75826f0cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc004cedf43d441faf040980b00f4590",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec31fbfea41249d1846a7ef868b165fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/256M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Load DistilBERT\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', normalize=True)\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ad1db79-4889-49cc-8b96-9d5fae6a6832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4: Prepare the model for prediction\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45ca5b84-5eb8-4bd8-8e2d-2989cc48acf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Split your data\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(text_data, location_labels, test_size=0.3, random_state=42)\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a637345c-fd90-4aaa-9bdf-a3f55a65995d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize location labels\n",
    "scaler = MinMaxScaler()\n",
    "train_labels_normalized = scaler.fit_transform(train_labels)\n",
    "val_labels_normalized = scaler.transform(val_labels)\n",
    "test_labels_normalized = scaler.transform(test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8092eb16-b696-4880-98af-9cf68ad98905",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - Train Loss: 0.0205 - Val Loss: 0.0193\n",
      "Epoch 2/5 - Train Loss: 0.0193 - Val Loss: 0.0193\n",
      "Epoch 3/5 - Train Loss: 0.0186 - Val Loss: 0.0195\n",
      "Epoch 4/5 - Train Loss: 0.0179 - Val Loss: 0.0196\n",
      "Epoch 5/5 - Train Loss: 0.0171 - Val Loss: 0.0196\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Train the model\n",
    "class LocationDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=512,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "        return {\n",
    "            'input_ids': input_ids.to(device),\n",
    "            'attention_mask': attention_mask.to(device),\n",
    "            'label': torch.tensor(label, dtype=torch.float).to(device)\n",
    "        }\n",
    "\n",
    "train_dataset = LocationDataset(train_texts, train_labels_normalized)\n",
    "val_dataset = LocationDataset(val_texts, val_labels_normalized)\n",
    "\n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-4)  # Adjust the learning rate here\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['label']\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits.squeeze()\n",
    "        loss = loss_fn(logits, labels)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    avg_train_loss = train_loss / len(train_dataloader)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            input_ids = batch['input_ids']\n",
    "            attention_mask = batch['attention_mask']\n",
    "            labels = batch['label']\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits.squeeze()\n",
    "            loss = loss_fn(logits, labels)\n",
    "            val_loss += loss.item()\n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.4f} - Val Loss: {avg_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac828335-6bee-47e7-912f-1b7bb856cdd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test Loss: 0.019488021182379578\n",
      "Mean Squared Error (MSE): 144.5557037549921\n",
      "Mean Absolute Error (MAE): 8.688388651353918\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Evaluate the model on the test set\n",
    "test_dataset = LocationDataset(test_texts, test_labels_normalized)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['label']\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits.squeeze()\n",
    "        loss = loss_fn(logits, labels)\n",
    "        test_loss += loss.item()\n",
    "        # Convert logits to predicted labels\n",
    "        predicted_labels = logits.detach().cpu().numpy()\n",
    "        predictions.extend(predicted_labels)\n",
    "\n",
    "avg_test_loss = test_loss / len(test_dataloader)\n",
    "print(f\"Average Test Loss: {avg_test_loss}\")\n",
    "\n",
    "# Convert predictions and labels back to original scale\n",
    "predictions = scaler.inverse_transform(predictions)\n",
    "test_labels = scaler.inverse_transform(test_labels_normalized)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mse = mean_squared_error(test_labels, predictions)\n",
    "mae = mean_absolute_error(test_labels, predictions)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb91c86-c0fa-4c57-a91e-da65e448a58c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc9ad07-d2f0-4b30-99f2-716c3d3dae2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3078f8d9-6126-424f-8fd0-3d7997aef0e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83e58e0-44cf-4e55-a2c4-4256ad7eecb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc84b12c-7d3d-4243-a3fc-436c6461fbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 6: Train the model\n",
    "# class LocationDataset(Dataset):\n",
    "#     def __init__(self, texts, labels):\n",
    "#         self.texts = texts\n",
    "#         self.labels = labels\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.texts)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         text = self.texts[idx]\n",
    "#         label = self.labels[idx]\n",
    "#         encoding = tokenizer.encode_plus(\n",
    "#             text,\n",
    "#             add_special_tokens=True,\n",
    "#             max_length=512,\n",
    "#             padding='max_length',\n",
    "#             truncation=True,\n",
    "#             return_tensors='pt'\n",
    "#         )\n",
    "#         input_ids = encoding['input_ids'].squeeze()\n",
    "#         attention_mask = encoding['attention_mask'].squeeze()\n",
    "#         return {\n",
    "#             'input_ids': input_ids.to(device),\n",
    "#             'attention_mask': attention_mask.to(device),\n",
    "#             'label': torch.tensor(label, dtype=torch.float).to(device)\n",
    "#         }\n",
    "\n",
    "# train_dataset = LocationDataset(train_texts, train_labels)\n",
    "# val_dataset = LocationDataset(val_texts, val_labels)\n",
    "# test_dataset = LocationDataset(test_texts, test_labels)\n",
    "\n",
    "# batch_size = 8\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "# loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# num_epochs = 5\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     train_loss = 0.0\n",
    "#     for batch in train_dataloader:\n",
    "#         optimizer.zero_grad()\n",
    "#         input_ids = batch['input_ids']\n",
    "#         attention_mask = batch['attention_mask']\n",
    "#         labels = batch['label']\n",
    "#         outputs = model(input_ids, attention_mask=attention_mask)\n",
    "#         logits = outputs.logits.squeeze()\n",
    "#         loss = loss_fn(logits, labels)\n",
    "#         train_loss += loss.item()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#     avg_train_loss = train_loss / len(train_dataloader)\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs} - Average Training Loss: {avg_train_loss}\")\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67f19288-6665-4752-97f1-c1f92f5ad745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 7: Evaluate the model\n",
    "# model.eval()\n",
    "# test_loss = 0.0\n",
    "# predictions = []\n",
    "# with torch.no_grad():\n",
    "#     for batch in test_dataloader:\n",
    "#         input_ids = batch['input_ids']\n",
    "#         attention_mask = batch['attention_mask']\n",
    "#         labels = batch['label']\n",
    "#         outputs = model(input_ids, attention_mask=attention_mask)\n",
    "#         logits = outputs.logits.squeeze()\n",
    "#         loss = loss_fn(logits, labels)\n",
    "#         test_loss += loss.item()\n",
    "#         # Convert logits to predicted labels\n",
    "#         predicted_labels = logits.detach().cpu().numpy()\n",
    "#         predictions.extend(predicted_labels)\n",
    "\n",
    "# avg_test_loss = test_loss / len(test_dataloader)\n",
    "# print(f\"Average Test Loss: {avg_test_loss}\")\n",
    "\n",
    "# # Calculate accuracy\n",
    "# predictions = np.array(predictions)\n",
    "# predicted_locations = predictions.round()  # Round the predicted values to the nearest integers\n",
    "# test_locations = test_labels  # Assuming you have the true labels stored in 'test_labels'\n",
    "\n",
    "# # Compute accuracy\n",
    "# accuracy = (predicted_locations == test_locations).mean()\n",
    "# print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d73693-7376-4374-939d-0cd4e41e4b5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eed2afd-059c-408b-83a5-d1162852d160",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911182f1-2a81-412a-9953-d7ba6e805d11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364f1807-2ff2-486b-8ef5-2601ab410303",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728d7cb9-0445-493d-b545-ea94a79830db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705eedc0-f28a-4fed-935c-52b3901d3371",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d642ebca-ead2-4216-81a1-22b611c99cb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85be8494-4ab4-449a-982d-15ce6b0e78de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e7320b-ce6e-4ab3-ada2-6b4a152d6720",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
